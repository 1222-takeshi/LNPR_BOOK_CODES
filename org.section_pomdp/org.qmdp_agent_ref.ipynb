{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "from puddle_world import *\n",
    "from mcl import *\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QmdpAgent(MclAgent):\n",
    "    def __init__(self, goal, puddles, time_interval, pf, sampling_num=10 , widths=np.array([0.2, 0.2, math.pi/18]).T, \\\n",
    "                 lowerleft=np.array([-4, -4]).T, upperright=np.array([4, 4]).T): \n",
    "        super().__init__(time_interval, 0.0, 0.0, pf)\n",
    "        \n",
    "        self.goal = goal\n",
    "        self.pose_min = np.r_[lowerleft, 0] \n",
    "        self.pose_max = np.r_[upperright, math.pi*2]\n",
    "        self.widths = widths\n",
    "        self.index_nums = ((self.pose_max - self.pose_min)/self.widths).astype(int)\n",
    "        nx, ny, nt = self.index_nums\n",
    "        self.indexes = list(itertools.product(range(nx), range(ny), range(nt)))\n",
    "        \n",
    "        self.value_function = self.init_value()\n",
    "        \n",
    "        policy = self.init_policy()\n",
    "        self.actions = list(set([tuple(policy[i]) for i in self.indexes]))\n",
    "        \n",
    "        self.state_transition_probs = self.init_state_transition_probs(time_interval, sampling_num)\n",
    "        self.depths = self.depth_means(puddles, sampling_num)        \n",
    "        \n",
    "    def init_value(self):\n",
    "        tmp = np.zeros(np.r_[self.index_nums])\n",
    "        for line in open(\"value.txt\", \"r\"):\n",
    "            d = line.split()\n",
    "            tmp[int(d[0]), int(d[1]), int(d[2])] = float(d[3])\n",
    "            \n",
    "        return tmp\n",
    "    \n",
    "    def init_policy(self):\n",
    "        tmp = np.zeros(np.r_[self.index_nums,2]) #制御出力が2次元なので、配列の次元を4次元に\n",
    "        for index in self.indexes:\n",
    "            center = self.pose_min + self.widths*(np.array(index).T + 0.5)  #セルの中心の座標\n",
    "            tmp[index] = PuddleIgnoreAgent.policy(center, self.goal)\n",
    "            \n",
    "        return tmp\n",
    "    \n",
    "    def init_state_transition_probs(self, time_interval, sampling_num):\n",
    "        ###セルの中の座標を均等にsampling_num**3点サンプリング###\n",
    "        dx = np.linspace(0.001, self.widths[0]*0.999, sampling_num) #隣のセルにはみ出さないように端を避ける\n",
    "        dy = np.linspace(0.001, self.widths[1]*0.999, sampling_num)\n",
    "        dt = np.linspace(0.001, self.widths[2]*0.999, sampling_num)\n",
    "        samples = list(itertools.product(dx, dy, dt))\n",
    "        \n",
    "        ###各行動、各方角でサンプリングした点を移動してインデックスの増分を記録###\n",
    "        tmp = {}\n",
    "        for a in self.actions:\n",
    "            for i_t in range(self.index_nums[2]):\n",
    "                transitions = []\n",
    "                for s in samples:\n",
    "                    before = np.array([s[0], s[1], s[2] + i_t*self.widths[2]]).T + self.pose_min  #遷移前の姿勢\n",
    "                    before_index = np.array([0, 0, i_t]).T                                                      #遷移前のインデックス\n",
    "                \n",
    "                    after = IdealRobot.state_transition(a[0], a[1], time_interval, before)   #遷移後の姿勢\n",
    "                    after_index = np.floor((after - self.pose_min)/self.widths).astype(int)   #遷移後のインデックス\n",
    "                    \n",
    "                    transitions.append(after_index - before_index)                                  #インデックスの差分を追加\n",
    "                    \n",
    "                unique, count = np.unique(transitions, axis=0, return_counts=True)   #集計（どのセルへの遷移が何回か）\n",
    "                probs = [c/sampling_num**3 for c in count]                   #サンプル数で割って確率にする\n",
    "                tmp[a,i_t] = list(zip(unique, probs))\n",
    "                \n",
    "        return tmp\n",
    "    \n",
    "    def depth_means(self, puddles, sampling_num):\n",
    "        ###セルの中の座標を均等にsampling_num**2点サンプリング###\n",
    "        dx = np.linspace(0, self.widths[0], sampling_num) \n",
    "        dy = np.linspace(0, self.widths[1], sampling_num)\n",
    "        samples = list(itertools.product(dx, dy))\n",
    "        \n",
    "        tmp = np.zeros(self.index_nums[0:2]) #深さの合計が計算されて入る\n",
    "        for xy in itertools.product(range(self.index_nums[0]), range(self.index_nums[1])):\n",
    "            for s in samples:\n",
    "                pose = self.pose_min + self.widths*np.array([xy[0], xy[1], 0]).T + np.array([s[0], s[1], 0]).T #セルの中心の座標\n",
    "                for p in puddles:\n",
    "                    tmp[xy] += p.depth*p.inside(pose) #深さに水たまりの中か否か（1 or 0）をかけて足す\n",
    "                        \n",
    "            tmp[xy] /= sampling_num**2 #深さの合計から平均値に変換\n",
    "                       \n",
    "        return tmp\n",
    "        \n",
    "    def policy(self, pose): #姿勢から離散状態のインデックスを作って方策を参照して返すだけ\n",
    "        for p in self.pf.particles:\n",
    "            index = np.floor((p.pose - self.pose_min)/self.widths).astype(int) \n",
    "            index[2] = (index[2] + self.index_nums[2]*1000)%self.index_nums[2] #角度の正規化\n",
    "            for i in [0,1]:   \n",
    "                if index[i] < 0: index[i] = 0\n",
    "                elif index[i] >= self.index_nums[i]: index[i] = self.index_nums[i] - 1\n",
    "                    \n",
    "            cur_value = self.value_function[tuple(index)]\n",
    "            next_values = [ self.action_value(a, index) for a in self.actions ]\n",
    "\n",
    "            print(cur_value, next_values)\n",
    "        \n",
    "        return (0.2, math.pi/20)\n",
    "\n",
    "    def action_value(self, action, index): #はみ出しペナルティー追加\n",
    "        value = 0.0\n",
    "        for delta, prob in self.state_transition_probs[(action, index[2])]: \n",
    "            after, edge_reward = self.edge_correction(np.array(index).T + delta)\n",
    "            after = tuple(after)\n",
    "            reward = - self.time_interval * self.depths[(after[0], after[1])] * self.puddle_coef - self.time_interval + edge_reward\n",
    "            value += (self.value_function[after] + reward) * prob\n",
    "\n",
    "        return value\n",
    "            \n",
    "    def edge_correction(self, index): #変更\n",
    "        edge_reward = 0.0\n",
    "        index[2] = (index[2] + self.index_nums[2])%self.index_nums[2] #方角の処理\n",
    "        \n",
    "        for i in range(2):\n",
    "            if index[i] < 0:\n",
    "                index[i] = 0\n",
    "                edge_reward = -1e100\n",
    "            elif index[i] >= self.index_nums[i]:\n",
    "                index[i] = self.index_nums[i]-1\n",
    "                edge_reward = -1e100\n",
    "                \n",
    "        return index, edge_reward\n",
    "        \n",
    "    def decision(self, observation=None):\n",
    "        self.pf.motion_update(self.prev_nu, self.prev_omega, self.time_interval)\n",
    "        self.pf.observation_update(observation)\n",
    "        \n",
    "        nu, omega = self.policy(self.pf.ml.pose)\n",
    "        self.prev_nu, self.prev_omega = nu, omega\n",
    "        return nu, omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QmdpAgent' object has no attribute 'puddle_coef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d4c94495f479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GIT/LectureOfProbabilisticRobotics/scripts/ideal_robot.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_span\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             self.ani = anm.FuncAnimation(fig, self.one_step, fargs=(elems, ax),\n",
      "\u001b[0;32m~/GIT/LectureOfProbabilisticRobotics/scripts/puddle_world.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, i, elems, ax)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m###puddleworld4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpuddle_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpuddle_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/LectureOfProbabilisticRobotics/scripts/ideal_robot.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, i, elems, ax)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"one_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/LectureOfProbabilisticRobotics/scripts/robot.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, time_interval)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msensor\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstuck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-672ce854ff46>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_nu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_omega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-672ce854ff46>\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(self, pose)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mcur_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mnext_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-672ce854ff46>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mcur_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mnext_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-672ce854ff46>\u001b[0m in \u001b[0;36maction_value\u001b[0;34m(self, action, index)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_interval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpuddle_coef\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_interval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0medge_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QmdpAgent' object has no attribute 'puddle_coef'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  ###dppolicyagentrun\n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(0.1, time_interval, debug=True) \n",
    "\n",
    "    m = Map()\n",
    "#    m.append_landmark(Landmark(-4,2))\n",
    "#    m.append_landmark(Landmark(2,-3))\n",
    "#    m.append_landmark(Landmark(4,4))\n",
    "    m.append_landmark(Landmark(-4,-4))\n",
    "    world.append(m)\n",
    "    \n",
    "    ###ゴールの追加###\n",
    "    goal = Goal(-3,-3)\n",
    "    world.append(goal)\n",
    "    \n",
    "    ###水たまりの追加###\n",
    "    puddles = [Puddle((-2, 0), (0, 2), 0.1), Puddle((-0.5, -2), (2.5, 1), 0.1)] \n",
    "    world.append(puddles[0]) \n",
    "    world.append(puddles[1])\n",
    "    \n",
    "    ### いくつかの初期位置を定義 ###   ###dppolicyagentrun\n",
    "\n",
    "    init_pose = np.array([3, 3, 0]).T\n",
    "    pf = Mcl(m, init_pose, 100)\n",
    "    a = QmdpAgent(Goal(-3,-3), puddles, time_interval, pf)\n",
    "    r = Robot(init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0), \n",
    "              agent=a, color=\"red\", bias_rate_stds=(0,0))\n",
    "\n",
    "    world.append(r)\n",
    "        \n",
    "    world.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
