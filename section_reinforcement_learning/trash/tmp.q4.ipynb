{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "from dp_policy_agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateInfo: \n",
    "    def __init__(self, action_num, epsilon=0.3):\n",
    "        self.action_values = np.zeros(action_num)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def greedy(self):\n",
    "        return np.argmax(self.action_values)\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(range(len(self.action_values)))\n",
    "        else:\n",
    "            return self.greedy()\n",
    "    \n",
    "    def get_action(self):\n",
    "        return self.epsilon_greedy(self.epsilon)\n",
    "    \n",
    "    def get_state_value(self):\n",
    "        return max(self.action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(PuddleIgnoreAgent):  \n",
    "    def __init__(self, time_interval, particle_pose, envmap, goal, particle_num=100, \\\n",
    "                motion_noise_stds={\"nn\":0.19, \"no\":0.001, \"on\":0.13, \"oo\":0.2}, widths=np.array([0.2, 0.2, math.pi/18]).T, \\\n",
    "                lowerleft=np.array([-4, -4]).T, upperright=np.array([4, 4]).T, puddle_coef=100, alpha=0.5):    #alpha追加\n",
    "        \n",
    "        super().__init__(time_interval, particle_pose, envmap, goal, particle_num, motion_noise_stds)\n",
    "        \n",
    "        ###DynamicProgrammingから持ってくる###\n",
    "        self.pose_min = np.r_[lowerleft, 0]\n",
    "        self.pose_max = np.r_[upperright, math.pi*2]\n",
    "        self.widths = widths\n",
    "        self.goal = goal\n",
    "        self.index_nums = ((self.pose_max - self.pose_min)/self.widths).astype(int)\n",
    "        nx, ny, nt = self.index_nums\n",
    "        self.indexes = list(itertools.product(range(nx), range(ny), range(nt)))\n",
    "        \n",
    "        self.time_interval = time_interval\n",
    "        self.puddle_coef = puddle_coef\n",
    "        \n",
    "        ###PuddleWorldから水たまりとゴールの情報をもらうための変数###\n",
    "        self.puddle_depth = 0.0\n",
    "        self.in_goal = False\n",
    "        self.final_value = 0.0\n",
    "        \n",
    "        ###PuddleIgnorePolicyの方策と価値関数の読み込み###\n",
    "        self.actions, self.action_value_function = self.set_action_value_function()\n",
    "        \n",
    "        self.s, self.a, self.r, self.prev_q, self.after_q = None, None, None, 0, 0 #追加\n",
    "        self.alpha = alpha #追加\n",
    "\n",
    "    def set_action_value_function(self): #二つのファイルを読んで行動のリストと行動価値関数を初期化\n",
    "        policy = np.zeros(np.r_[self.index_nums,2])\n",
    "        for line in open(\"puddle_ignore_policy.txt\", \"r\"): #方策のファイルを読み込む\n",
    "            d = line.split()\n",
    "            policy[int(d[0]), int(d[1]), int(d[2])] = [float(d[3]), float(d[4])]\n",
    "        \n",
    "        actions = list(set([tuple(policy[i]) for i in self.indexes])) #行動のリスト（前進、左回転、右回転のリストに）\n",
    "        action_num = len(actions) #行動のリストの要素数\n",
    "        \n",
    "        q = {}\n",
    "        for line in open(\"puddle_ignore_values.txt\", \"r\"): #価値のファイルを読み込む\n",
    "            d = line.split()\n",
    "            index, value = (int(d[0]), int(d[1]), int(d[2])), float(d[3]) #インデックスをタプル、値を数字に\n",
    "            q[index] = StateInfo(action_num) #StateInfoオブジェクトを割り当てて初期化\n",
    "            \n",
    "            for i, a in enumerate(actions): #方策の行動価値を価値のファイルに書いてある値に。そうでない場合はちょっと引く\n",
    "                q[index].action_values[i] = value if tuple(policy[index]) == a else value - 0.1\n",
    "                \n",
    "        return actions, q\n",
    "    \n",
    "    def policy(self, pose): ###q4\n",
    "        index = np.floor((pose - self.pose_min)/self.widths).astype(int)\n",
    "        \n",
    "        index[2] = (index[2] + self.index_nums[2]*1000)%self.index_nums[2]\n",
    "        for i in [0,1]:\n",
    "            if index[i] < 0: index[i] = 0\n",
    "            elif index[i] >= self.index_nums[i]: index[i] = self.index_nums[i] - 1\n",
    "            \n",
    "        s = tuple(index) #Q学習の式で使う記号に変更\n",
    "        a = self.action_value_function[s].get_action()\n",
    "        return s, a\n",
    "        \n",
    "    def decision(self, observation=None): ###q4\n",
    "        self.r = -self.time_interval - self.time_interval * self.puddle_coef * self.puddle_depth #前回の状態遷移の報酬\n",
    "        \n",
    "        self.mcl.motion_update(self.prev_nu, self.prev_omega, self.time_interval)\n",
    "        self.mcl.observation_update(observation)\n",
    "        s_dash, a_next = self.policy(self.mcl.ml_pose) #MCLの結果から前回の状態遷移のs'と次の行動（max_a'のa'ではないことに注意）を観測\n",
    "        \n",
    "        if self.s != None:\n",
    "            next_q = self.action_value_function[s_dash].get_state_value()\n",
    "            self.prev_q = self.action_value_function[self.s].action_values[self.a]\n",
    "            \n",
    "            self.action_value_function[self.s].action_values[self.a] = self.alpha*self.action_value_function[self.s].action_values[self.a] \\\n",
    "                                                                                               + (1.0 - self.alpha)*(self.r + next_q)\n",
    "                \n",
    "            self.after_q = self.action_value_function[self.s].action_values[self.a]\n",
    "        \n",
    "        self.s, self.a = s_dash, a_next #今の状態と行動対を次の状態遷移元にセット\n",
    "        self.prev_nu, self.prev_omega = self.actions[a_next]\n",
    "        return self.actions[a_next]\n",
    "    \n",
    "    def draw(self, ax, elems): \n",
    "        super().draw(ax, elems)\n",
    "        elems.append(ax.text(-4, -4.5, \"reward:\" + str(self.r), fontsize=8))\n",
    "        elems.append(ax.text(-1.5, -4.5, \"Q delta:\" + str(self.after_q - self.prev_q), fontsize=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PuddleRobot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b9cfa07eaa68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0minit_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_pose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     r = PuddleRobot(time_interval, init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0),\n\u001b[0m\u001b[1;32m     24\u001b[0m               agent=qa, color=\"red\", bias_rate_stds=(0,0))\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PuddleRobot' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(400, time_interval)  #長めにアニメーション時間をとる\n",
    "\n",
    "    m = Map()\n",
    "    m.append_landmark(Landmark(-4,2))\n",
    "    m.append_landmark(Landmark(2,-3))\n",
    "    m.append_landmark(Landmark(3,3))\n",
    "    m.append_landmark(Landmark(-4,-4)) #追加（ゴール方向にランドマークがないので）\n",
    "    world.append(m)\n",
    "    \n",
    "    ###ゴールの追加###\n",
    "    goal = Goal(-3,-3)\n",
    "    world.append(goal)\n",
    "    \n",
    "    ###水たまりの追加###\n",
    "    world.append(Puddle((-2, 0), (0, 2), 0.1)) \n",
    "    world.append(Puddle((-0.5, -2), (2.5, 1), 0.1))\n",
    "\n",
    "    ###ロボットを1台登場させる###\n",
    "    init_pose = np.array([3, 3, 0]).T\n",
    "    qa = QAgent(time_interval, init_pose, m, goal)  \n",
    "    r = PuddleRobot(time_interval, init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0),\n",
    "              agent=qa, color=\"red\", bias_rate_stds=(0,0))\n",
    "\n",
    "    world.append(r)\n",
    "    \n",
    "    world.draw()\n",
    "    #r.one_step(0.1) #デバッグ時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros(qa.index_nums[0:2])\n",
    "for x in range(qa.index_nums[0]):\n",
    "    for y in range(qa.index_nums[1]):\n",
    "        a = qa.action_value_function[(x,y,22)].greedy()\n",
    "        p[x,y] = qa.actions[a][0] + qa.actions[a][1]\n",
    "        \n",
    "import seaborn as sns   \n",
    "sns.heatmap(np.rot90(p), square=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
