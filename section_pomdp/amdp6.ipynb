{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../scripts/')\n",
    "from dynamic_programming import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeliefDynamicProgramming(DynamicProgramming):\n",
    "    def __init__(self, widths, goal, puddles, time_interval, sampling_num, camera, puddle_coef=100.0, \\\n",
    "                 lowerleft=np.array([-4, -4]).T, upperright=np.array([4, 4]).T, dev_borders=[0.1,0.2,0.4,0.8]): \n",
    "        super().__init__(widths, goal, puddles, time_interval, sampling_num, puddle_coef, lowerleft, upperright)  ###amdp6changeactions（4-9行目）\n",
    "        \n",
    "        self.actions = [(0.0, 2.0), (0.0, -2.0), (1.0, 0.0), (-1.0, 0.0)] #バック（-1.0, 0.0）を追加してself.actionsを再定義（この位置に！）\n",
    "        self.state_transition_probs = self.init_state_transition_probs(time_interval, sampling_num) #追加。計算し直し。\n",
    "        \n",
    "        self.index_nums = np.array([*self.index_nums, len(dev_borders) + 1])\n",
    "        nx, ny, nt, nh = self.index_nums\n",
    "        self.indexes = list(itertools.product(range(nx), range(ny), range(nt), range(nh)))\n",
    "        \n",
    "        self.value_function, self.final_state_flags =  self.init_belief_value_function()\n",
    "        self.policy = np.zeros(np.r_[self.index_nums,2]) \n",
    "        \n",
    "        self.dev_borders = dev_borders\n",
    "        self.dev_borders_side = [dev_borders[0]/10, *dev_borders, dev_borders[-1]*10]\n",
    "        self.motion_sigma_transition_probs = self.init_motion_sigma_transition_probs()\n",
    "        self.obs_sigma_transition_probs = self.init_obs_sigma_transition_probs(camera) #追加\n",
    "\n",
    "    def init_obs_sigma_transition_probs(self, camera):\n",
    "        probs = {}\n",
    "        for index in self.indexes: \n",
    "            pose = self.pose_min + self.widths*(np.array(index[0:3]).T + 0.5)   \n",
    "            sigma = (self.dev_borders_side[index[3]] + self.dev_borders_side[index[3]+1])/2\n",
    "            S = (sigma**2)*np.eye(3)\n",
    "\n",
    "            for d in camera.data(pose):\n",
    "                S = self.observation_update(d[1], S, camera, pose)\n",
    "                    \n",
    "            probs[index] = {self.cov_to_index(S):1.0}\n",
    "\n",
    "        return probs\n",
    "    \n",
    "    def observation_update(self, landmark_id, S, camera, pose):\n",
    "        distance_dev_rate = 0.14\n",
    "        direction_dev = 0.05\n",
    "        \n",
    "        H = matH(pose, camera.map.landmarks[landmark_id].pos)\n",
    "        estimated_z = IdealCamera.observation_function(pose, camera.map.landmarks[landmark_id].pos)\n",
    "        Q = matQ(distance_dev_rate*estimated_z[0], direction_dev)\n",
    "        K = S.dot(H.T).dot(np.linalg.inv(Q + H.dot(S).dot(H.T)))\n",
    "        return (np.eye(3) - K.dot(H)).dot(S)\n",
    "        \n",
    "    def init_motion_sigma_transition_probs(self):\n",
    "        probs = {}\n",
    "        for a in self.actions:\n",
    "            for i in range(len(self.dev_borders)+1):\n",
    "                probs[(i, a)] = self.calc_motion_sigma_transition_probs(self.dev_borders_side[i], self.dev_borders_side[i+1], a)\n",
    "                \n",
    "        return probs\n",
    "            \n",
    "    def cov_to_index(self, cov):\n",
    "        sigma = np.power(np.linalg.det(cov), 1.0/6)\n",
    "        for i, e in enumerate(self.dev_borders):\n",
    "            if sigma < e: return i\n",
    "            \n",
    "        return len(self.dev_borders)\n",
    "        \n",
    "    def calc_motion_sigma_transition_probs(self, min_sigma, max_sigma, action, sampling_num=100):\n",
    "        nu, omega = action\n",
    "        if abs(omega) < 1e-5: omega = 1e-5\n",
    "\n",
    "        F = matF(nu, omega, self.time_interval, 0.0) #ロボットの向きは関係ないので0[deg]で固定で\n",
    "        M = matM(nu, omega, self.time_interval, {\"nn\":0.19, \"no\":0.001, \"on\":0.13, \"oo\":0.2})#移動の誤差モデル（カルマンフィルタのものをコピペ）\n",
    "        A = matA(nu, omega, self.time_interval, 0.0)\n",
    "        \n",
    "        ans = {}\n",
    "        for sigma in np.linspace(min_sigma, max_sigma*0.999, sampling_num): #遷移前のσを作る（区間内に一様分布していると仮定）\n",
    "            index_after = self.cov_to_index(sigma*sigma*F.dot(F.T) + A.dot(M).dot(A.T)) #遷移後のσのインデックス\n",
    "            ans[index_after] = 1 if index_after not in ans else ans[index_after] + 1 #単にカウントしてるだけ（辞書の初期化もあるのでややこしい）\n",
    "                \n",
    "        for e in ans:\n",
    "            ans[e] /= sampling_num #頻度を確率に\n",
    "\n",
    "        return ans\n",
    "    \n",
    "    def init_belief_value_function(self): \n",
    "        v = np.empty(self.index_nums)\n",
    "        f = np.zeros(self.index_nums) \n",
    "        \n",
    "        for index in self.indexes:\n",
    "            f[index] = self.belief_final_state(np.array(index).T)\n",
    "            v[index] = self.goal.value if f[index] else -100.0\n",
    "                \n",
    "        return v, f\n",
    "        \n",
    "    def belief_final_state(self, index):\n",
    "        x_min, y_min, _ = self.pose_min + self.widths*index[0:3] \n",
    "        x_max, y_max, _ = self.pose_min + self.widths*(index[0:3] + 1) \n",
    "        \n",
    "        corners = [[x_min, y_min, _], [x_min, y_max, _], [x_max, y_min, _], [x_max, y_max, _] ] \n",
    "        return all([self.goal.inside(np.array(c).T) for c in corners ]) and index[3] == 0\n",
    "    \n",
    "    def action_value(self, action, index, out_penalty=True):###amdp6\n",
    "        value = 0.0\n",
    "        for delta, prob in self.state_transition_probs[(action, index[2])]:\n",
    "            after, out_reward = self.out_correction(np.array(index[0:3]).T + delta)\n",
    "        \n",
    "            reward = - self.time_interval * self.depths[(after[0], after[1])] * self.puddle_coef - self.time_interval + out_reward*out_penalty\n",
    "            for sigma_after, sigma_prob in self.motion_sigma_transition_probs[(index[3], action)].items():\n",
    "                for sigma_obs, sigma_obs_prob in dp.obs_sigma_transition_probs[(*after, sigma_after)].items(): #もう一段追加\n",
    "                    value += (self.value_function[(*after, sigma_obs)] + reward) * prob * sigma_prob * sigma_obs_prob #確率の掛け算も追加\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "puddles = [Puddle((-2, 0), (0, 2), 0.1), Puddle((-0.5, -2), (2.5, 1), 0.1)]\n",
    "\n",
    "##地図とカメラを作る##\n",
    "m = Map()\n",
    "for ln in [(1,4), (4,1), (-4, 1), (-2, 1)]: m.append_landmark(Landmark(*ln))\n",
    "c = IdealCamera(m)\n",
    "    \n",
    "dp = BeliefDynamicProgramming(np.array([0.2, 0.2, math.pi/18]).T, Goal(-3,-3), puddles, 0.1, 10, c) #カメラを加える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 99.89247284552096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fcded319ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_iteration_sweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/LectureOfProbabilisticRobotics/scripts/dynamic_programming.py\u001b[0m in \u001b[0;36mvalue_iteration_sweep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmax_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mmax_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#全行動の行動価値を計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mmax_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m                               \u001b[0;31m#最大の行動価値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mmax_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m#最大の行動価値を与える行動\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/LectureOfProbabilisticRobotics/scripts/dynamic_programming.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmax_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mmax_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#全行動の行動価値を計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mmax_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m                               \u001b[0;31m#最大の行動価値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mmax_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m#最大の行動価値を与える行動\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8da900ef7219>\u001b[0m in \u001b[0;36maction_value\u001b[0;34m(self, action, index, out_penalty)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_transition_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_interval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpuddle_coef\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_interval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_reward\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def save():\n",
    "    with open(\"policy_amdp.txt\", \"w\") as f:  ###amdp6sweeps\n",
    "        for index in dp.indexes:\n",
    "            p = dp.policy[index]\n",
    "            f.write(\"{} {} {} {} {} {}\\n\".format(index[0], index[1], index[2],index[3], p[0], p[1])) #一つ{}とindexの要素を増やす\n",
    "\n",
    "    with open(\"value_amdp.txt\", \"w\") as f:\n",
    "        for index in dp.indexes:\n",
    "            p = dp.value_function[index]\n",
    "            f.write(\"{} {} {} {} {}\\n\".format(index[0], index[1], index[2], index[3], p)) #5行目と同じ\n",
    "\n",
    "delta = 1e100\n",
    "counter = 0\n",
    "\n",
    "while delta > 0.01: \n",
    "    delta = dp.value_iteration_sweep()\n",
    "    counter += 1\n",
    "    print(counter, delta)\n",
    "    save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "v = dp.value_function[:, :, 18, 4]\n",
    "sns.heatmap(np.rot90(v), square=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
